{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"âœ… Running in Google Colab\")\n",
        "\n",
        "    # Mount Google Drive for persistent storage\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Create project directory in Google Drive\n",
        "    import os\n",
        "    PROJECT_DIR = '/content/drive/My Drive/Resume_Analyzer'\n",
        "    os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "    os.chdir(PROJECT_DIR)\n",
        "    print(f\"ðŸ“ Project directory: {PROJECT_DIR}\")\n",
        "\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    PROJECT_DIR = '.'\n",
        "    print(\"âš ï¸ Not running in Colab - using local directory\")\n"
      ],
      "metadata": {
        "id": "xhplirMXUs11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q numpy pandas scikit-learn scipy tqdm matplotlib seaborn spacy fastapi uvicorn python-multipart nest-asyncio sentence-transformers transformers\n",
        "!pip install pdfplumber python-docx textstat\n",
        "\n"
      ],
      "metadata": {
        "id": "N6OuJYuJUuMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import io\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP and ML\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Document processing\n",
        "import pdfplumber\n",
        "from docx import Document\n",
        "import textstat\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm  # Auto-detect Colab\n",
        "\n",
        "# Colab specific imports\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()  # Allow nested event loops in Colab\n",
        "\n",
        "# Kaggle dataset\n",
        "import kagglehub\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "logger = logging.getLogger(\"ResumeAnalyzer\")\n",
        "\n",
        "@dataclass\n",
        "class ColabConfig:\n",
        "    \"\"\"Colab-optimized configuration\"\"\"\n",
        "\n",
        "    # Model architecture (smaller for Colab memory limits)\n",
        "    SENTENCE_TRANSFORMER = \"sentence-transformers/all-MiniLM-L6-v2\"  # Smaller model\n",
        "    EMBEDDING_DIM = 384  # Reduced for memory\n",
        "    HIDDEN_DIMS = [256, 128, 64]  # Smaller network\n",
        "    DROPOUT_RATES = [0.3, 0.2, 0.1]\n",
        "\n",
        "    # Training parameters (Colab-friendly)\n",
        "    BATCH_SIZE = 8  # Smaller batch size for Colab\n",
        "    LEARNING_RATE = 2e-5\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    EPOCHS = 20  # Fewer epochs for faster training\n",
        "    GRADIENT_CLIP = 1.0\n",
        "\n",
        "    # Data augmentation\n",
        "    AUGMENTATION_FACTOR = 2  # Less augmentation for speed\n",
        "    NOISE_FACTOR = 0.05\n",
        "\n",
        "    # Feature weights\n",
        "    FEATURE_WEIGHTS = {\n",
        "        'skills': 0.25, 'experience': 0.20, 'education': 0.15,\n",
        "        'achievements': 0.15, 'formatting': 0.10, 'keywords': 0.10, 'readability': 0.05\n",
        "    }\n",
        "\n",
        "    # Device and paths\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    NUM_WORKERS = 2  # Colab-friendly\n",
        "\n",
        "    # Paths in Google Drive\n",
        "    MODEL_DIR = os.path.join(PROJECT_DIR, \"models\")\n",
        "    DATA_DIR = os.path.join(PROJECT_DIR, \"data\")\n",
        "    CACHE_DIR = os.path.join(PROJECT_DIR, \"cache\")\n",
        "\n",
        "    def __post_init__(self):\n",
        "        os.makedirs(self.MODEL_DIR, exist_ok=True)\n",
        "        os.makedirs(self.DATA_DIR, exist_ok=True)\n",
        "        os.makedirs(self.CACHE_DIR, exist_ok=True)\n",
        "\n",
        "config = ColabConfig()\n",
        "\n",
        "print(f\"ðŸ”§ Colab Configuration:\")\n",
        "print(f\"   Device: {config.DEVICE}\")\n",
        "print(f\"   Embedding Dim: {config.EMBEDDING_DIM}\")\n",
        "print(f\"   Batch Size: {config.BATCH_SIZE}\")\n",
        "print(f\"   Project Dir: {PROJECT_DIR}\")\n",
        "\n",
        "# GPU Memory check\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"   GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "    # Clear cache\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "29WCZ7n5U4XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import io\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "from docx import Document\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class ColabDocumentParser:\n",
        "    \"\"\"Colab-optimized document parser with safe handling\"\"\"\n",
        "\n",
        "    SUPPORTED_FORMATS = {'.pdf', '.docx', '.txt'}\n",
        "\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "            logger.info(\"âœ… SpaCy model loaded successfully\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"SpaCy model loading failed: {e}\")\n",
        "            self.nlp = None\n",
        "\n",
        "    def parse_pdf(self, file_content: bytes) -> Dict:\n",
        "        try:\n",
        "            text_parts = []\n",
        "            with pdfplumber.open(io.BytesIO(file_content)) as pdf:\n",
        "                for page in pdf.pages[:5]:  # limit to 5 pages for memory\n",
        "                    text = page.extract_text() or \"\"\n",
        "                    if text.strip():\n",
        "                        text_parts.append(text)\n",
        "            full_text = \"\\n\".join(text_parts)\n",
        "            return {'text': full_text, 'word_count': len(full_text.split()), 'pages': len(text_parts), 'success': True}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"PDF parsing failed: {e}\")\n",
        "            return {'text': '', 'word_count': 0, 'pages': 0, 'success': False}\n",
        "\n",
        "    def parse_docx(self, file_content: bytes) -> Dict:\n",
        "        try:\n",
        "            doc = Document(io.BytesIO(file_content))\n",
        "            text_parts = [p.text for p in doc.paragraphs if p.text.strip()]\n",
        "            full_text = \"\\n\".join(text_parts)\n",
        "            return {'text': full_text, 'word_count': len(full_text.split()), 'paragraphs': len(text_parts), 'success': True}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"DOCX parsing failed: {e}\")\n",
        "            return {'text': '', 'word_count': 0, 'paragraphs': 0, 'success': False}\n",
        "\n",
        "    def parse_text(self, file_content: bytes) -> Dict:\n",
        "        try:\n",
        "            text = file_content.decode('utf-8')\n",
        "            return {'text': text, 'word_count': len(text.split()), 'success': True}\n",
        "        except Exception:\n",
        "            try:\n",
        "                text = file_content.decode('latin-1', errors='ignore')\n",
        "                return {'text': text, 'word_count': len(text.split()), 'success': True}\n",
        "            except Exception as e2:\n",
        "                logger.error(f\"Text parsing failed: {e2}\")\n",
        "                return {'text': '', 'word_count': 0, 'success': False}\n",
        "\n",
        "    def parse_document(self, file_content: bytes, filename: str) -> Dict:\n",
        "        \"\"\"Main parsing method with full safety\"\"\"\n",
        "        # Step 1: Sanitize filename\n",
        "        clean_name = os.path.basename(filename)\n",
        "        clean_name = re.sub(r'[?\\\\]', '', clean_name).strip()\n",
        "\n",
        "        # Step 2: Validate extension\n",
        "        file_ext = Path(clean_name).suffix.lower()\n",
        "        if file_ext not in self.SUPPORTED_FORMATS:\n",
        "            return {'text': '', 'word_count': 0, 'success': False, 'error': f'Unsupported file format: {file_ext}'}\n",
        "\n",
        "        # Step 3: Parse document safely\n",
        "        try:\n",
        "            if file_ext == '.pdf':\n",
        "                result = self.parse_pdf(file_content)\n",
        "            elif file_ext == '.docx':\n",
        "                result = self.parse_docx(file_content)\n",
        "            elif file_ext == '.txt':\n",
        "                result = self.parse_text(file_content)\n",
        "            else:\n",
        "                # should never reach here due to validation\n",
        "                result = {'text': '', 'word_count': 0, 'success': False, 'error': 'Unknown format'}\n",
        "        except Exception as e:\n",
        "            return {'text': '', 'word_count': 0, 'success': False, 'error': f'Parsing failed: {e}'}\n",
        "\n",
        "        # Step 4: Enrich result if parsing succeeded\n",
        "        if result.get('success') and result.get('text'):\n",
        "            result['contact'] = self.extract_contact_info(result['text'])\n",
        "            result['sections'] = self._identify_sections(result['text'])\n",
        "            result['filename'] = clean_name\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    import re\n",
        "\n",
        "    def extract_contact_info(self, text: str):\n",
        "        contact_info = {}\n",
        "\n",
        "        # Email\n",
        "        email_match = re.search(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", text)\n",
        "        if email_match:\n",
        "            contact_info['email'] = email_match.group(0)\n",
        "\n",
        "        # Phone (supports +91, +1, (123) 456-7890, etc.)\n",
        "        phone_match = re.search(r\"(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{2,4}\\)?[-.\\s]?\\d{3,4}[-.\\s]?\\d{3,4}\", text)\n",
        "        if phone_match:\n",
        "            contact_info['phone'] = phone_match.group(0)\n",
        "\n",
        "        # LinkedIn\n",
        "        linkedin_match = re.search(r\"(linkedin\\.com\\/in\\/[A-Za-z0-9_-]+)\", text, re.IGNORECASE)\n",
        "        if linkedin_match:\n",
        "            contact_info['linkedin'] = linkedin_match.group(0)\n",
        "\n",
        "        return contact_info\n",
        "\n",
        "    def _identify_sections(self, text: str) -> List[str]:\n",
        "        \"\"\"Identify sections safely\"\"\"\n",
        "        section_patterns = {\n",
        "            'contact': r'(email|phone|address|linkedin)',\n",
        "            'summary': r'(summary|objective|profile)',\n",
        "            'experience': r'(experience|employment|work\\s+history)',\n",
        "            'education': r'(education|academic|degree)',\n",
        "            'skills': r'(skills|competencies|technical)',\n",
        "            'projects': r'(projects|portfolio)',\n",
        "            'achievements': r'(achievements|awards|honors)'\n",
        "        }\n",
        "        found_sections = []\n",
        "        text_lower = text.lower()\n",
        "        for section, pattern in section_patterns.items():\n",
        "            if re.search(pattern, text_lower):\n",
        "                found_sections.append(section)\n",
        "        return found_sections\n",
        "\n",
        "# Test the parser\n",
        "parser = ColabDocumentParser()\n",
        "print(\"âœ… Document parser initialized safely\")\n"
      ],
      "metadata": {
        "id": "ADdJT9asVCz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColabFeatureExtractor:\n",
        "    \"\"\"Colab-optimized feature extractor\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Load smaller sentence transformer for Colab\n",
        "        try:\n",
        "            self.sentence_transformer = SentenceTransformer(config.SENTENCE_TRANSFORMER)\n",
        "            logger.info(f\"âœ… Sentence transformer loaded: {config.SENTENCE_TRANSFORMER}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load sentence transformer: {e}\")\n",
        "            self.sentence_transformer = None\n",
        "\n",
        "        # Load spaCy\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except:\n",
        "            self.nlp = None\n",
        "\n",
        "        # Skill database\n",
        "        self.skill_database = {\n",
        "            'programming': ['python', 'java', 'javascript', 'sql', 'r', 'c++', 'c#', 'php'],\n",
        "            'data_science': ['machine learning', 'tensorflow', 'pytorch', 'pandas', 'numpy', 'scikit-learn'],\n",
        "            'web_dev': ['react', 'angular', 'vue', 'node.js', 'html', 'css', 'bootstrap'],\n",
        "            'cloud': ['aws', 'azure', 'gcp', 'docker', 'kubernetes'],\n",
        "            'databases': ['mysql', 'postgresql', 'mongodb', 'redis'],\n",
        "            'soft_skills': ['leadership', 'communication', 'teamwork', 'problem solving']\n",
        "        }\n",
        "\n",
        "    def extract_skills(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract skills by category\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        found_skills = {}\n",
        "\n",
        "        for category, skills in self.skill_database.items():\n",
        "            category_skills = []\n",
        "            for skill in skills:\n",
        "                pattern = r'\\\\b' + re.escape(skill.lower()) + r'\\\\b'\n",
        "                if re.search(pattern, text_lower):\n",
        "                    category_skills.append(skill)\n",
        "\n",
        "            if category_skills:\n",
        "                found_skills[category] = category_skills\n",
        "\n",
        "        return found_skills\n",
        "\n",
        "    def extract_experience_years(self, text: str) -> float:\n",
        "        \"\"\"Extract years of experience\"\"\"\n",
        "        patterns = [\n",
        "            r'(\\\\d+)\\\\s*\\\\+?\\\\s*years?\\\\s*of\\\\s*experience',\n",
        "            r'(\\\\d+)\\\\s*\\\\+?\\\\s*yrs?\\\\s*experience',\n",
        "            r'experience.*?(\\\\d+)\\\\s*\\\\+?\\\\s*years?',\n",
        "        ]\n",
        "\n",
        "        years = []\n",
        "        for pattern in patterns:\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            years.extend([int(match) for match in matches])\n",
        "\n",
        "        # Also try to extract from date ranges\n",
        "        date_pattern = r'\\\\b(20\\\\d{2}|19\\\\d{2})\\\\b'\n",
        "        dates = [int(date) for date in re.findall(date_pattern, text)]\n",
        "\n",
        "        calculated_years = 0\n",
        "        if len(dates) >= 2:\n",
        "            calculated_years = max(dates) - min(dates)\n",
        "\n",
        "        # Return the maximum found\n",
        "        all_years = years + [calculated_years]\n",
        "        return max(all_years) if all_years else 0.0\n",
        "\n",
        "    def extract_education_level(self, text: str) -> int:\n",
        "        \"\"\"Extract education level (0-4 scale)\"\"\"\n",
        "        education_patterns = {\n",
        "            4: [r'ph\\\\.?d\\\\.?', r'doctorate', r'doctoral'],\n",
        "            3: [r\"master's?\", r'm\\\\.s\\\\.?', r'm\\\\.a\\\\.?', r'mba'],\n",
        "            2: [r\"bachelor's?\", r'b\\\\.s\\\\.?', r'b\\\\.a\\\\.?', r'undergraduate'],\n",
        "            1: [r'associate', r'a\\\\.s\\\\.?', r'a\\\\.a\\\\.?']\n",
        "        }\n",
        "\n",
        "        text_lower = text.lower()\n",
        "        max_level = 0\n",
        "\n",
        "        for level, patterns in education_patterns.items():\n",
        "            for pattern in patterns:\n",
        "                if re.search(pattern, text_lower):\n",
        "                    max_level = max(max_level, level)\n",
        "\n",
        "        return max_level\n",
        "\n",
        "    def calculate_readability_score(self, text: str) -> float:\n",
        "        \"\"\"Calculate readability score\"\"\"\n",
        "        try:\n",
        "            if len(text) < 50:\n",
        "                return 50.0\n",
        "\n",
        "            flesch_score = textstat.flesch_reading_ease(text)\n",
        "            return flesch_score if flesch_score is not None else 50.0\n",
        "        except:\n",
        "            return 50.0\n",
        "\n",
        "    def get_text_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Get text embedding using sentence transformer\"\"\"\n",
        "        if not self.sentence_transformer or not text.strip():\n",
        "            return np.zeros(config.EMBEDDING_DIM, dtype=np.float32)\n",
        "\n",
        "        try:\n",
        "            embedding = self.sentence_transformer.encode(text, convert_to_numpy=True)\n",
        "            # Ensure correct dimension\n",
        "            if embedding.shape[0] != config.EMBEDDING_DIM:\n",
        "                # Pad or truncate to match expected dimension\n",
        "                if embedding.shape[0] > config.EMBEDDING_DIM:\n",
        "                    embedding = embedding[:config.EMBEDDING_DIM]\n",
        "                else:\n",
        "                    padded = np.zeros(config.EMBEDDING_DIM)\n",
        "                    padded[:embedding.shape[0]] = embedding\n",
        "                    embedding = padded\n",
        "\n",
        "            return embedding.astype(np.float32)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Embedding failed: {e}\")\n",
        "            return np.zeros(config.EMBEDDING_DIM, dtype=np.float32)\n",
        "\n",
        "    def extract_all_features(self, document: Dict, job_description: str = \"\") -> Dict:\n",
        "        \"\"\"Extract comprehensive features from document\"\"\"\n",
        "        text = document.get('text', '')\n",
        "\n",
        "        if not text.strip():\n",
        "            return self._get_empty_features()\n",
        "\n",
        "        # Extract various features\n",
        "        skills = self.extract_skills(text)\n",
        "        experience_years = self.extract_experience_years(text)\n",
        "        education_level = self.extract_education_level(text)\n",
        "        readability = self.calculate_readability_score(text)\n",
        "\n",
        "        # Calculate derived metrics\n",
        "        total_skills = sum(len(skill_list) for skill_list in skills.values())\n",
        "        skill_diversity = len(skills.keys()) / 6.0 if skills else 0.0  # Max 6 categories\n",
        "\n",
        "        # Contact completeness\n",
        "        contact = document.get('contact', {})\n",
        "        contact_score = 0.0\n",
        "        if contact.get('email'): contact_score += 0.4\n",
        "        if contact.get('phone'): contact_score += 0.4\n",
        "        if contact.get('linkedin'): contact_score += 0.1\n",
        "        if contact.get('github'): contact_score += 0.1\n",
        "\n",
        "        # Section completeness\n",
        "        sections = document.get('sections', [])\n",
        "        required_sections = ['experience', 'education', 'skills']\n",
        "        section_score = len(set(sections) & set(required_sections)) / len(required_sections)\n",
        "\n",
        "        # ATS friendliness\n",
        "        word_count = document.get('word_count', 0)\n",
        "        ats_score = 0.0\n",
        "        if word_count > 200: ats_score += 0.3\n",
        "        if contact.get('email'): ats_score += 0.2\n",
        "        ats_score += section_score * 0.3\n",
        "        ats_score += min(1.0, word_count / 1000) * 0.2\n",
        "\n",
        "        # Keyword density (if job description provided)\n",
        "        keyword_density = 0.0\n",
        "        if job_description:\n",
        "            job_words = set(job_description.lower().split())\n",
        "            resume_words = set(text.lower().split())\n",
        "            common_words = job_words & resume_words\n",
        "            keyword_density = len(common_words) / max(len(job_words), 1)\n",
        "\n",
        "        # Get embeddings\n",
        "        doc_embedding = self.get_text_embedding(text)\n",
        "\n",
        "        # Section embeddings (simplified for Colab)\n",
        "        section_embeddings = {}\n",
        "        for section in ['experience', 'education', 'skills']:\n",
        "            if section in text.lower():\n",
        "                # Extract section text (simplified)\n",
        "                section_start = text.lower().find(section)\n",
        "                section_text = text[section_start:section_start+500] if section_start >= 0 else \"\"\n",
        "                section_embeddings[section] = self.get_text_embedding(section_text)\n",
        "            else:\n",
        "                section_embeddings[section] = np.zeros(config.EMBEDDING_DIM, dtype=np.float32)\n",
        "\n",
        "        return {\n",
        "            # Basic metrics\n",
        "            'word_count': word_count,\n",
        "            'section_count': len(sections),\n",
        "            'experience_years': experience_years,\n",
        "            'education_level': education_level,\n",
        "            'total_skills': total_skills,\n",
        "\n",
        "            # Calculated scores\n",
        "            'skill_diversity': skill_diversity,\n",
        "            'contact_completeness': contact_score,\n",
        "            'section_completeness': section_score,\n",
        "            'ats_friendliness': ats_score,\n",
        "            'readability_score': readability / 100.0,  # Normalize to 0-1\n",
        "            'keyword_density': keyword_density,\n",
        "\n",
        "            # Binary features\n",
        "            'has_email': 1.0 if contact.get('email') else 0.0,\n",
        "            'has_phone': 1.0 if contact.get('phone') else 0.0,\n",
        "            'has_linkedin': 1.0 if contact.get('linkedin') else 0.0,\n",
        "            'has_achievements': 1.0 if 'award' in text.lower() or 'achievement' in text.lower() else 0.0,\n",
        "\n",
        "            # Embeddings\n",
        "            'document_embedding': doc_embedding,\n",
        "            'experience_embedding': section_embeddings.get('experience', np.zeros(config.EMBEDDING_DIM)),\n",
        "            'education_embedding': section_embeddings.get('education', np.zeros(config.EMBEDDING_DIM)),\n",
        "            'skills_embedding': section_embeddings.get('skills', np.zeros(config.EMBEDDING_DIM)),\n",
        "\n",
        "            # Additional info\n",
        "            'skills_found': skills,\n",
        "            'sections_found': sections,\n",
        "            'contact_info': contact\n",
        "        }\n",
        "\n",
        "    def _get_empty_features(self) -> Dict:\n",
        "        \"\"\"Return empty features for failed parsing\"\"\"\n",
        "        return {\n",
        "            'word_count': 0, 'section_count': 0, 'experience_years': 0.0,\n",
        "            'education_level': 0, 'total_skills': 0, 'skill_diversity': 0.0,\n",
        "            'contact_completeness': 0.0, 'section_completeness': 0.0,\n",
        "            'ats_friendliness': 0.0, 'readability_score': 0.5, 'keyword_density': 0.0,\n",
        "            'has_email': 0.0, 'has_phone': 0.0, 'has_linkedin': 0.0, 'has_achievements': 0.0,\n",
        "            'document_embedding': np.zeros(config.EMBEDDING_DIM),\n",
        "            'experience_embedding': np.zeros(config.EMBEDDING_DIM),\n",
        "            'education_embedding': np.zeros(config.EMBEDDING_DIM),\n",
        "            'skills_embedding': np.zeros(config.EMBEDDING_DIM),\n",
        "            'skills_found': {}, 'sections_found': [], 'contact_info': {}\n",
        "        }\n",
        "\n",
        "# Initialize feature extractor\n",
        "extractor = ColabFeatureExtractor()\n",
        "print(\"âœ… Feature extractor initialized\")\n"
      ],
      "metadata": {
        "id": "3-54q4ZgVLTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColabResumeScorer(nn.Module):\n",
        "    \"\"\"Colab-optimized resume scoring model\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int = config.EMBEDDING_DIM,\n",
        "                 hidden_dims: List[int] = config.HIDDEN_DIMS,\n",
        "                 dropout_rates: List[float] = config.DROPOUT_RATES,\n",
        "                 num_attention_heads: int = 4):  # Reduced for Colab\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Input projections for different embeddings\n",
        "        self.document_proj = nn.Linear(embedding_dim, hidden_dims[0])\n",
        "        self.experience_proj = nn.Linear(embedding_dim, hidden_dims[0])\n",
        "        self.education_proj = nn.Linear(embedding_dim, hidden_dims[0])\n",
        "        self.skills_proj = nn.Linear(embedding_dim, hidden_dims[0])\n",
        "\n",
        "        # Numeric features processor (14 numeric features)\n",
        "        self.numeric_features_dim = 14\n",
        "        self.numeric_proj = nn.Sequential(\n",
        "            nn.Linear(self.numeric_features_dim, hidden_dims[0] // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rates[0])\n",
        "        )\n",
        "\n",
        "        # Multi-head attention for feature fusion\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_dims[0],\n",
        "            num_heads=num_attention_heads,\n",
        "            dropout=dropout_rates[0],\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Feature fusion layers\n",
        "        fusion_input_dim = hidden_dims[0] * 4 + hidden_dims[0] // 2\n",
        "\n",
        "        self.fusion_layers = nn.Sequential(\n",
        "            nn.Linear(fusion_input_dim, hidden_dims[1]),\n",
        "            nn.LayerNorm(hidden_dims[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rates[1]),\n",
        "\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
        "            nn.LayerNorm(hidden_dims[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rates[2])\n",
        "        )\n",
        "\n",
        "        # Output heads\n",
        "        final_dim = hidden_dims[2]\n",
        "\n",
        "        self.skills_head = nn.Sequential(\n",
        "            nn.Linear(final_dim, final_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(final_dim // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.experience_head = nn.Sequential(\n",
        "            nn.Linear(final_dim, final_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(final_dim // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.education_head = nn.Sequential(\n",
        "            nn.Linear(final_dim, final_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(final_dim // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.formatting_head = nn.Sequential(\n",
        "            nn.Linear(final_dim, final_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(final_dim // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.achievements_head = nn.Sequential(\n",
        "            nn.Linear(final_dim, final_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(final_dim // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Overall score head\n",
        "        self.overall_head = nn.Sequential(\n",
        "            nn.Linear(final_dim, final_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(final_dim // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, embeddings_dict: Dict[str, torch.Tensor], numeric_features: torch.Tensor):\n",
        "        batch_size = numeric_features.shape[0]\n",
        "\n",
        "        # Project embeddings\n",
        "        doc_emb = self.document_proj(embeddings_dict['document'])\n",
        "        exp_emb = self.experience_proj(embeddings_dict['experience'])\n",
        "        edu_emb = self.education_proj(embeddings_dict['education'])\n",
        "        skills_emb = self.skills_proj(embeddings_dict['skills'])\n",
        "\n",
        "        # Process numeric features\n",
        "        numeric_features = torch.nan_to_num(numeric_features, nan=0.0)\n",
        "        numeric_emb = self.numeric_proj(numeric_features)\n",
        "\n",
        "        # Stack embeddings for attention\n",
        "        embedding_stack = torch.stack([doc_emb, exp_emb, edu_emb, skills_emb], dim=1)\n",
        "\n",
        "        # Apply attention\n",
        "        attended_emb, attention_weights = self.attention(\n",
        "            embedding_stack, embedding_stack, embedding_stack\n",
        "        )\n",
        "        attended_emb = attended_emb.mean(dim=1)  # Pool\n",
        "\n",
        "        # Concatenate all features\n",
        "        fused_features = torch.cat([\n",
        "            doc_emb, exp_emb, edu_emb, skills_emb, numeric_emb\n",
        "        ], dim=1)\n",
        "\n",
        "        # Pass through fusion layers\n",
        "        x = self.fusion_layers(fused_features)\n",
        "\n",
        "        # Generate scores\n",
        "        scores = {\n",
        "            'skills': self.skills_head(x).squeeze(-1),\n",
        "            'experience': self.experience_head(x).squeeze(-1),\n",
        "            'education': self.education_head(x).squeeze(-1),\n",
        "            'formatting': self.formatting_head(x).squeeze(-1),\n",
        "            'achievements': self.achievements_head(x).squeeze(-1),\n",
        "            'overall': self.overall_head(x).squeeze(-1)\n",
        "        }\n",
        "\n",
        "        return scores, attention_weights\n",
        "\n",
        "print(\"âœ… Deep learning model defined\")\n"
      ],
      "metadata": {
        "id": "qZ9wdJXWVUN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColabResumeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, features_list: List[Dict], scores_list: List[Dict],\n",
        "                 scaler: Optional[StandardScaler] = None, augment: bool = False):\n",
        "\n",
        "        self.features_list = features_list\n",
        "        self.scores_list = scores_list\n",
        "\n",
        "        # Numeric feature keys (14 features)\n",
        "        self.numeric_keys = [\n",
        "            'word_count', 'section_count', 'experience_years', 'education_level',\n",
        "            'total_skills', 'skill_diversity', 'contact_completeness', 'section_completeness',\n",
        "            'ats_friendliness', 'readability_score', 'keyword_density',\n",
        "            'has_email', 'has_phone', 'has_linkedin'\n",
        "        ]\n",
        "\n",
        "        # Build numeric matrix\n",
        "        self.numeric_matrix, self.scaler = self._build_numeric_matrix(scaler)\n",
        "\n",
        "        # Process embeddings\n",
        "        self.embeddings_dict = self._process_embeddings()\n",
        "\n",
        "        # Process targets\n",
        "        self.targets = self._process_targets()\n",
        "\n",
        "        # Apply augmentation if requested\n",
        "        if augment:\n",
        "            self._apply_colab_augmentation()\n",
        "\n",
        "        logger.info(f\"Dataset initialized: {len(self)} samples\")\n",
        "\n",
        "    def _build_numeric_matrix(self, scaler):\n",
        "        numeric_data = []\n",
        "\n",
        "        for features in self.features_list:\n",
        "            row = []\n",
        "            for key in self.numeric_keys:\n",
        "                value = features.get(key, 0.0)\n",
        "                # Handle special cases\n",
        "                if key == 'word_count':\n",
        "                    value = min(value, 2000)  # Cap at 2000\n",
        "                elif key == 'experience_years':\n",
        "                    value = min(value, 20)  # Cap at 20 years\n",
        "                elif key == 'total_skills':\n",
        "                    value = min(value, 30)  # Cap at 30 skills\n",
        "\n",
        "                row.append(float(value))\n",
        "            numeric_data.append(row)\n",
        "\n",
        "        numeric_matrix = np.array(numeric_data, dtype=np.float32)\n",
        "        numeric_matrix = np.nan_to_num(numeric_matrix, nan=0.0)\n",
        "\n",
        "        # Scale features\n",
        "        if scaler is None:\n",
        "            scaler = StandardScaler()\n",
        "            numeric_matrix = scaler.fit_transform(numeric_matrix)\n",
        "        else:\n",
        "            numeric_matrix = scaler.transform(numeric_matrix)\n",
        "\n",
        "        return numeric_matrix.astype(np.float32), scaler\n",
        "\n",
        "    def _process_embeddings(self):\n",
        "        embedding_types = ['document', 'experience', 'education', 'skills']\n",
        "        embeddings_dict = {}\n",
        "\n",
        "        for emb_type in embedding_types:\n",
        "            embeddings = []\n",
        "            for features in self.features_list:\n",
        "                key = f'{emb_type}_embedding'\n",
        "                emb = features.get(key, np.zeros(config.EMBEDDING_DIM))\n",
        "\n",
        "                # Ensure correct shape\n",
        "                if emb.shape[0] != config.EMBEDDING_DIM:\n",
        "                    emb = np.zeros(config.EMBEDDING_DIM)\n",
        "\n",
        "                embeddings.append(emb)\n",
        "\n",
        "            embeddings_dict[emb_type] = np.array(embeddings, dtype=np.float32)\n",
        "\n",
        "        return embeddings_dict\n",
        "\n",
        "    def _process_targets(self):\n",
        "        targets = []\n",
        "        target_keys = ['skills', 'experience', 'education', 'formatting', 'achievements', 'overall']\n",
        "\n",
        "        for scores in self.scores_list:\n",
        "            target_row = []\n",
        "            for key in target_keys:\n",
        "                value = scores.get(key, 0.5)  # Default to middle score\n",
        "                value = max(0.0, min(1.0, value))  # Clamp to [0, 1]\n",
        "                target_row.append(value)\n",
        "            targets.append(target_row)\n",
        "\n",
        "        return np.array(targets, dtype=np.float32)\n",
        "\n",
        "    def _apply_colab_augmentation(self):\n",
        "        \"\"\"Light augmentation for Colab memory constraints\"\"\"\n",
        "        original_size = len(self.features_list)\n",
        "\n",
        "        # Only augment a subset to save memory\n",
        "        augment_size = min(original_size, 200)  # Limit augmentation\n",
        "\n",
        "        for i in range(augment_size):\n",
        "            # Add noise to numeric features\n",
        "            noisy_numeric = self.numeric_matrix[i] + np.random.normal(0, 0.05, self.numeric_matrix[i].shape)\n",
        "            self.numeric_matrix = np.vstack([self.numeric_matrix, noisy_numeric])\n",
        "\n",
        "            # Add noise to embeddings\n",
        "            for emb_type in self.embeddings_dict:\n",
        "                noisy_emb = self.embeddings_dict[emb_type][i] + np.random.normal(0, 0.02, self.embeddings_dict[emb_type][i].shape)\n",
        "                self.embeddings_dict[emb_type] = np.vstack([self.embeddings_dict[emb_type], noisy_emb])\n",
        "\n",
        "            # Copy targets with small noise\n",
        "            noisy_target = self.targets[i] + np.random.normal(0, 0.01, self.targets[i].shape)\n",
        "            noisy_target = np.clip(noisy_target, 0, 1)\n",
        "            self.targets = np.vstack([self.targets, noisy_target])\n",
        "\n",
        "        logger.info(f\"Augmented dataset from {original_size} to {len(self.targets)} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embeddings = {}\n",
        "        for emb_type in self.embeddings_dict:\n",
        "            embeddings[emb_type] = torch.tensor(self.embeddings_dict[emb_type][idx], dtype=torch.float32)\n",
        "\n",
        "        numeric = torch.tensor(self.numeric_matrix[idx], dtype=torch.float32)\n",
        "        target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
        "\n",
        "        return {'embeddings': embeddings, 'numeric': numeric, 'target': target}\n",
        "\n",
        "class ColabTrainer:\n",
        "    \"\"\"Colab-optimized trainer with memory management\"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, device=config.DEVICE):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=3, factor=0.5)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        # Training history\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def compute_loss(self, predictions, targets):\n",
        "        target_keys = ['skills', 'experience', 'education', 'formatting', 'achievements', 'overall']\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for i, key in enumerate(target_keys):\n",
        "            if key in predictions:\n",
        "                loss = self.criterion(predictions[key], targets[:, i])\n",
        "                total_loss += loss\n",
        "\n",
        "        return total_loss / len(target_keys)\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "            # Clear gradients and cache\n",
        "            self.optimizer.zero_grad()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Move to device\n",
        "            embeddings = {k: v.to(self.device) for k, v in batch['embeddings'].items()}\n",
        "            numeric = batch['numeric'].to(self.device)\n",
        "            targets = batch['target'].to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            predictions, _ = self.model(embeddings, numeric)\n",
        "            loss = self.compute_loss(predictions, targets)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        return total_loss / num_batches\n",
        "\n",
        "    def validate_epoch(self, val_loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                embeddings = {k: v.to(self.device) for k, v in batch['embeddings'].items()}\n",
        "                numeric = batch['numeric'].to(self.device)\n",
        "                targets = batch['target'].to(self.device)\n",
        "\n",
        "                predictions, _ = self.model(embeddings, numeric)\n",
        "                loss = self.compute_loss(predictions, targets)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "        return total_loss / num_batches\n",
        "\n",
        "    def fit(self, train_loader, val_loader, epochs=config.EPOCHS):\n",
        "        logger.info(f\"Starting training for {epochs} epochs...\")\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            train_loss = self.train_epoch(train_loader)\n",
        "            val_loss = self.validate_epoch(val_loader)\n",
        "\n",
        "            # Update scheduler\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Store history\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            # Print progress\n",
        "            if epoch % 2 == 0:  # Print every 2 epochs\n",
        "                logger.info(f\"Epoch {epoch+1}/{epochs} - Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(self.model.state_dict(), os.path.join(config.MODEL_DIR, \"best_model.pth\"))\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= 5:  # Early stopping\n",
        "                    logger.info(\"Early stopping triggered\")\n",
        "                    break\n",
        "\n",
        "        # Load best model\n",
        "        self.model.load_state_dict(torch.load(os.path.join(config.MODEL_DIR, \"best_model.pth\")))\n",
        "        logger.info(f\"Training completed. Best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "        return self.train_losses, self.val_losses\n",
        "\n",
        "print(\"âœ… Dataset and trainer classes defined\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2phcsnDOVbh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_kaggle_resume_data(max_samples=500):\n",
        "    \"\"\"Load resume data from Kaggle with Colab optimization\"\"\"\n",
        "    try:\n",
        "        # Download dataset\n",
        "        logger.info(\"Downloading Kaggle resume dataset...\")\n",
        "        path = kagglehub.dataset_download(\"gauravduttakiit/resume-dataset\")\n",
        "        df = pd.read_csv(os.path.join(path, \"UpdatedResumeDataSet.csv\"))\n",
        "\n",
        "        # Sample data for Colab memory constraints\n",
        "        if len(df) > max_samples:\n",
        "            df = df.sample(n=max_samples, random_state=42)\n",
        "\n",
        "        logger.info(f\"Loaded {len(df)} resumes\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load Kaggle data: {e}\")\n",
        "        # Return sample data if Kaggle fails\n",
        "        return create_sample_data(max_samples)\n",
        "\n",
        "def create_sample_data(num_samples=100):\n",
        "    \"\"\"Create sample resume data for testing\"\"\"\n",
        "    logger.info(f\"Creating {num_samples} sample resumes...\")\n",
        "\n",
        "    sample_resumes = []\n",
        "    categories = ['Software Engineering', 'Data Science', 'Marketing', 'Sales', 'Design']\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        category = np.random.choice(categories)\n",
        "        years_exp = np.random.randint(1, 15)\n",
        "\n",
        "        # Generate sample resume text\n",
        "        resume_text = f\"\"\"\n",
        "        John Doe {i}\n",
        "        Email: john{i}@email.com\n",
        "        Phone: (555) 123-{i:04d}\n",
        "\n",
        "        PROFESSIONAL SUMMARY\n",
        "        {category} professional with {years_exp} years of experience.\n",
        "\n",
        "        EXPERIENCE\n",
        "        Senior {category.split()[0]} - TechCorp Inc. (2020-2024)\n",
        "        â€¢ Led team of {np.random.randint(3, 10)} professionals\n",
        "        â€¢ Increased efficiency by {np.random.randint(10, 50)}%\n",
        "        â€¢ Managed projects worth ${np.random.randint(100, 1000)}K\n",
        "        â€¢ Implemented {np.random.choice(['Python', 'JavaScript', 'SQL', 'React'])} solutions\n",
        "\n",
        "        EDUCATION\n",
        "        {np.random.choice(['Bachelor of Science', 'Master of Science'])} in {category.split()[0]}\n",
        "        State University - Graduated {2024 - years_exp + 2}\n",
        "\n",
        "        SKILLS\n",
        "        Technical: {', '.join(np.random.choice(['Python', 'Java', 'SQL', 'AWS', 'Docker', 'React'], 4))}\n",
        "        Soft Skills: Leadership, Communication, Problem Solving\n",
        "\n",
        "        ACHIEVEMENTS\n",
        "        â€¢ Won {np.random.choice(['Excellence', 'Innovation', 'Leadership'])} Award\n",
        "        â€¢ Published {np.random.randint(1, 5)} papers/articles\n",
        "        â€¢ Certified in {np.random.choice(['AWS', 'Google Cloud', 'Microsoft Azure'])}\n",
        "        \"\"\"\n",
        "\n",
        "        sample_resumes.append({\n",
        "            'Resume': resume_text.strip(),\n",
        "            'Category': category\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(sample_resumes)\n",
        "\n",
        "def generate_realistic_scores(text, category):\n",
        "    \"\"\"Generate realistic scores based on resume content\"\"\"\n",
        "    word_count = len(text.split())\n",
        "\n",
        "    # Base scores on content analysis\n",
        "    skills_keywords = ['python', 'java', 'sql', 'aws', 'react', 'machine learning', 'leadership']\n",
        "    skills_found = sum(1 for skill in skills_keywords if skill.lower() in text.lower())\n",
        "\n",
        "    # Generate scores with some randomness but based on content\n",
        "    skills_score = min(0.95, (skills_found / len(skills_keywords)) * 0.7 + 0.2 + np.random.normal(0, 0.1))\n",
        "    experience_score = min(0.95, (word_count / 600) * 0.6 + 0.25 + np.random.normal(0, 0.1))\n",
        "    education_score = min(0.95, 0.5 + np.random.normal(0, 0.15))\n",
        "    formatting_score = min(0.95, 0.6 + np.random.normal(0, 0.1))\n",
        "    achievements_score = min(0.95, 0.4 + np.random.normal(0, 0.15))\n",
        "\n",
        "    # Overall score as weighted average\n",
        "    overall_score = (\n",
        "        skills_score * 0.25 +\n",
        "        experience_score * 0.25 +\n",
        "        education_score * 0.15 +\n",
        "        formatting_score * 0.15 +\n",
        "        achievements_score * 0.20\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'skills': max(0.1, min(0.95, skills_score)),\n",
        "        'experience': max(0.1, min(0.95, experience_score)),\n",
        "        'education': max(0.1, min(0.95, education_score)),\n",
        "        'formatting': max(0.1, min(0.95, formatting_score)),\n",
        "        'achievements': max(0.1, min(0.95, achievements_score)),\n",
        "        'overall': max(0.1, min(0.95, overall_score))\n",
        "    }\n",
        "\n",
        "def process_resume_dataset(df, max_samples=None):\n",
        "    \"\"\"Process resume dataset with Colab optimization\"\"\"\n",
        "    if max_samples and len(df) > max_samples:\n",
        "        df = df.sample(n=max_samples, random_state=42)\n",
        "\n",
        "    features_list = []\n",
        "    scores_list = []\n",
        "\n",
        "    logger.info(\"Processing resumes...\")\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
        "        try:\n",
        "            # Create document structure\n",
        "            text = str(row.get('Resume', ''))\n",
        "            category = str(row.get('Category', 'Unknown'))\n",
        "\n",
        "            # Mock document for feature extraction\n",
        "            document = {\n",
        "                'text': text,\n",
        "                'word_count': len(text.split()),\n",
        "                'success': True,\n",
        "                'contact': {'email': 'test@email.com'},  # Mock contact\n",
        "                'sections': ['experience', 'education', 'skills']  # Mock sections\n",
        "            }\n",
        "\n",
        "            # Extract features\n",
        "            features = extractor.extract_all_features(document)\n",
        "\n",
        "            # Generate realistic scores\n",
        "            scores = generate_realistic_scores(text, category)\n",
        "\n",
        "            features_list.append(features)\n",
        "            scores_list.append(scores)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to process resume {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    logger.info(f\"Successfully processed {len(features_list)} resumes\")\n",
        "    return features_list, scores_list\n",
        "\n",
        "print(\"âœ… Data loading functions defined\")\n"
      ],
      "metadata": {
        "id": "uyeOT92RVlRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ðŸ“± CELL 8: Training Pipeline\n",
        "# ============================================================================\n",
        "\n",
        "def train_colab_model(max_samples=300, epochs=15, batch_size=6):\n",
        "    \"\"\"Complete training pipeline optimized for Colab\"\"\"\n",
        "\n",
        "    logger.info(\"ðŸš€ Starting Colab Resume Analyzer Training\")\n",
        "    logger.info(\"=\" * 50)\n",
        "\n",
        "    # Step 1: Load data\n",
        "    logger.info(\"ðŸ“Š Step 1: Loading data...\")\n",
        "    df = load_kaggle_resume_data(max_samples)\n",
        "\n",
        "    # Step 2: Process resumes\n",
        "    logger.info(\"ðŸ”„ Step 2: Processing resumes...\")\n",
        "    features_list, scores_list = process_resume_dataset(df, max_samples)\n",
        "\n",
        "    if len(features_list) == 0:\n",
        "        raise ValueError(\"No resumes processed successfully!\")\n",
        "\n",
        "    # Step 3: Create dataset\n",
        "    logger.info(\"ðŸ“¦ Step 3: Creating dataset...\")\n",
        "    dataset = ColabResumeDataset(features_list, scores_list, augment=True)\n",
        "\n",
        "    # Step 4: Split data\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # Step 5: Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    logger.info(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    # Step 6: Initialize model\n",
        "    logger.info(\"ðŸ§  Step 4: Initializing model...\")\n",
        "    model = ColabResumeScorer()\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    logger.info(f\"Model parameters: {total_params:,}\")\n",
        "\n",
        "    # Step 7: Train model\n",
        "    logger.info(\"ðŸ‹ï¸ Step 5: Training model...\")\n",
        "    trainer = ColabTrainer(model)\n",
        "    train_losses, val_losses = trainer.fit(train_loader, val_loader, epochs=epochs)\n",
        "\n",
        "    # Step 8: Save components\n",
        "    logger.info(\"ðŸ’¾ Step 6: Saving model and scaler...\")\n",
        "    torch.save(model.state_dict(), os.path.join(config.MODEL_DIR, \"colab_model.pth\"))\n",
        "\n",
        "    with open(os.path.join(config.MODEL_DIR, \"scaler.pkl\"), 'wb') as f:\n",
        "        pickle.dump(dataset.scaler, f)\n",
        "\n",
        "    # Step 9: Plot training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training History')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(val_losses)\n",
        "    plt.title('Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    logger.info(\"âœ… Training completed successfully!\")\n",
        "    logger.info(f\"ðŸ“ˆ Final validation loss: {min(val_losses):.4f}\")\n",
        "\n",
        "    return model, dataset, trainer\n",
        "\n",
        "print(\"âœ… Training pipeline defined\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfP4q_ODVsyw",
        "outputId": "c939acbd-c9eb-4b5f-a4db-3c149ea885ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training pipeline defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ðŸ“± CELL 9: Production Analyzer\n",
        "# ============================================================================\n",
        "\n",
        "class ColabResumeAnalyzer:\n",
        "    \"\"\"Production-ready resume analyzer for Colab\"\"\"\n",
        "\n",
        "    def __init__(self, model_path=None, scaler_path=None):\n",
        "        self.device = config.DEVICE\n",
        "        self.parser = ColabDocumentParser()\n",
        "        self.extractor = ColabFeatureExtractor()\n",
        "\n",
        "        # Load model\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            self.model = ColabResumeScorer()\n",
        "            self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
        "            self.model.eval()\n",
        "            logger.info(f\"âœ… Model loaded from {model_path}\")\n",
        "        else:\n",
        "            logger.warning(\"âš ï¸ No model loaded - predictions will be random\")\n",
        "            self.model = ColabResumeScorer()\n",
        "\n",
        "        # Load scaler\n",
        "        if scaler_path and os.path.exists(scaler_path):\n",
        "            with open(scaler_path, 'rb') as f:\n",
        "                self.scaler = pickle.load(f)\n",
        "            logger.info(f\"âœ… Scaler loaded from {scaler_path}\")\n",
        "        else:\n",
        "            logger.warning(\"âš ï¸ No scaler loaded\")\n",
        "            self.scaler = None\n",
        "\n",
        "    def analyze(self, file_path, job_description=\"\"):\n",
        "        \"\"\"Analyze resume from file path\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                file_content = f.read()\n",
        "            filename = os.path.basename(file_path)\n",
        "            return self.analyze_resume_bytes(file_content, filename, job_description)\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': f'File reading failed: {e}'}\n",
        "\n",
        "    def analyze_resume_bytes(self, file_content, filename, job_description=\"\"):\n",
        "        \"\"\"Analyze resume from bytes\"\"\"\n",
        "        try:\n",
        "            # Step 1: Parse document\n",
        "            document = self.parser.parse_document(file_content, filename)\n",
        "\n",
        "            if not document.get('success', False):\n",
        "                return {\n",
        "                    'success': False,\n",
        "                    'error': 'Document parsing failed',\n",
        "                    'details': document.get('error', 'Unknown parsing error')\n",
        "                }\n",
        "\n",
        "            # Step 2: Extract features\n",
        "            features = self.extractor.extract_all_features(document, job_description)\n",
        "\n",
        "            # Step 3: Prepare model inputs\n",
        "            model_inputs = self._prepare_model_inputs(features)\n",
        "\n",
        "            # Step 4: Get predictions\n",
        "            with torch.no_grad():\n",
        "                predictions, attention_weights = self.model(\n",
        "                    model_inputs['embeddings'],\n",
        "                    model_inputs['numeric']\n",
        "                )\n",
        "\n",
        "            # Step 5: Convert to scores (0-100)\n",
        "            scores = {}\n",
        "            for key, value in predictions.items():\n",
        "                scores[key] = float(value.cpu().item() * 100)\n",
        "\n",
        "            # Step 6: Generate analysis\n",
        "            analysis = self._generate_analysis(features, scores, document)\n",
        "\n",
        "            # Step 7: Generate recommendations\n",
        "            recommendations = self._generate_recommendations(features, scores, job_description)\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'filename': filename,\n",
        "                'scores': scores,\n",
        "                'analysis': analysis,\n",
        "                'recommendations': recommendations,\n",
        "                'document_info': {\n",
        "                    'word_count': document.get('word_count', 0),\n",
        "                    'sections_found': document.get('sections', []),\n",
        "                    'contact_info': document.get('contact', {})\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Analysis failed: {e}\")\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': f'Analysis failed: {str(e)}'\n",
        "            }\n",
        "\n",
        "    def _prepare_model_inputs(self, features):\n",
        "        \"\"\"Prepare features for model input\"\"\"\n",
        "        # Extract embeddings\n",
        "        embeddings = {}\n",
        "        for emb_type in ['document', 'experience', 'education', 'skills']:\n",
        "            key = f'{emb_type}_embedding'\n",
        "            emb = features.get(key, np.zeros(config.EMBEDDING_DIM))\n",
        "            embeddings[emb_type] = torch.tensor(emb, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # Extract numeric features\n",
        "        numeric_keys = [\n",
        "            'word_count', 'section_count', 'experience_years', 'education_level',\n",
        "            'total_skills', 'skill_diversity', 'contact_completeness', 'section_completeness',\n",
        "            'ats_friendliness', 'readability_score', 'keyword_density',\n",
        "            'has_email', 'has_phone', 'has_linkedin'\n",
        "        ]\n",
        "\n",
        "        numeric_values = []\n",
        "        for key in numeric_keys:\n",
        "            value = features.get(key, 0.0)\n",
        "            # Apply same caps as in training\n",
        "            if key == 'word_count':\n",
        "                value = min(value, 2000)\n",
        "            elif key == 'experience_years':\n",
        "                value = min(value, 20)\n",
        "            elif key == 'total_skills':\n",
        "                value = min(value, 30)\n",
        "            numeric_values.append(float(value))\n",
        "\n",
        "        numeric_tensor = torch.tensor(numeric_values, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # Scale if scaler available\n",
        "        if self.scaler:\n",
        "            numeric_np = numeric_tensor.numpy()\n",
        "            numeric_scaled = self.scaler.transform(numeric_np)\n",
        "            numeric_tensor = torch.tensor(numeric_scaled, dtype=torch.float32)\n",
        "\n",
        "        numeric_tensor = numeric_tensor.to(self.device)\n",
        "\n",
        "        return {'embeddings': embeddings, 'numeric': numeric_tensor}\n",
        "\n",
        "    def _generate_analysis(self, features, scores, document):\n",
        "        \"\"\"Generate comprehensive analysis\"\"\"\n",
        "        analysis = {\n",
        "            'strengths': [],\n",
        "            'weaknesses': [],\n",
        "            'key_metrics': {}\n",
        "        }\n",
        "\n",
        "        # Identify strengths and weaknesses\n",
        "        for component, score in scores.items():\n",
        "            if component == 'overall':\n",
        "                continue\n",
        "\n",
        "            if score >= 80:\n",
        "                analysis['strengths'].append(f\"Excellent {component} section (Score: {score:.1f})\")\n",
        "            elif score >= 65:\n",
        "                analysis['strengths'].append(f\"Good {component} presentation (Score: {score:.1f})\")\n",
        "            elif score < 50:\n",
        "                analysis['weaknesses'].append(f\"{component.title()} section needs improvement (Score: {score:.1f})\")\n",
        "            elif score < 65:\n",
        "                analysis['weaknesses'].append(f\"{component.title()} could be enhanced (Score: {score:.1f})\")\n",
        "\n",
        "        # Key metrics\n",
        "        analysis['key_metrics'] = {\n",
        "            'word_count': features.get('word_count', 0),\n",
        "            'experience_years': features.get('experience_years', 0),\n",
        "            'total_skills': features.get('total_skills', 0),\n",
        "            'ats_friendliness': f\"{features.get('ats_friendliness', 0) * 100:.1f}%\",\n",
        "            'sections_found': len(features.get('sections_found', [])),\n",
        "            'contact_complete': features.get('contact_completeness', 0) > 0.5\n",
        "        }\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def _generate_recommendations(self, features, scores, job_description):\n",
        "        \"\"\"Generate actionable recommendations\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        # Skills recommendations\n",
        "        if scores.get('skills', 0) < 70:\n",
        "            recommendations.append({\n",
        "                'category': 'Skills',\n",
        "                'priority': 'HIGH',\n",
        "                'action': 'Enhance technical skills section',\n",
        "                'details': [\n",
        "                    'Add more relevant programming languages',\n",
        "                    'Include industry-specific tools',\n",
        "                    'Group skills by category',\n",
        "                    'Add proficiency levels'\n",
        "                ]\n",
        "            })\n",
        "\n",
        "        # Experience recommendations\n",
        "        if scores.get('experience', 0) < 70:\n",
        "            recommendations.append({\n",
        "                'category': 'Experience',\n",
        "                'priority': 'HIGH',\n",
        "                'action': 'Improve experience descriptions',\n",
        "                'details': [\n",
        "                    'Use action verbs to start bullet points',\n",
        "                    'Quantify achievements with numbers',\n",
        "                    'Show career progression',\n",
        "                    'Include relevant keywords'\n",
        "                ]\n",
        "            })\n",
        "\n",
        "        # Education recommendations\n",
        "        if scores.get('education', 0) < 60:\n",
        "            recommendations.append({\n",
        "                'category': 'Education',\n",
        "                'priority': 'MEDIUM',\n",
        "                'action': 'Strengthen education section',\n",
        "                'details': [\n",
        "                    'Include relevant coursework',\n",
        "                    'Add academic achievements',\n",
        "                    'Mention certifications',\n",
        "                    'Include graduation honors'\n",
        "                ]\n",
        "            })\n",
        "\n",
        "        # Formatting recommendations\n",
        "        if scores.get('formatting', 0) < 65:\n",
        "            recommendations.append({\n",
        "                'category': 'Formatting',\n",
        "                'priority': 'MEDIUM',\n",
        "                'action': 'Improve ATS compatibility',\n",
        "                'details': [\n",
        "                    'Use standard section headers',\n",
        "                    'Avoid complex formatting',\n",
        "                    'Ensure text is selectable',\n",
        "                    'Use consistent formatting'\n",
        "                ]\n",
        "            })\n",
        "\n",
        "        # Job matching\n",
        "        if job_description and features.get('keyword_density', 0) < 0.2:\n",
        "            recommendations.append({\n",
        "                'category': 'Job Matching',\n",
        "                'priority': 'HIGH',\n",
        "                'action': 'Better align with job requirements',\n",
        "                'details': [\n",
        "                    'Include more job-specific keywords',\n",
        "                    'Highlight relevant experience',\n",
        "                    'Customize skills for this role',\n",
        "                    'Match required qualifications'\n",
        "                ]\n",
        "            })\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def get_ats_grade(self, overall_score):\n",
        "        \"\"\"Get ATS compatibility grade\"\"\"\n",
        "        if overall_score >= 90:\n",
        "            return \"A - Excellent\"\n",
        "        elif overall_score >= 80:\n",
        "            return \"B - Very Good\"\n",
        "        elif overall_score >= 70:\n",
        "            return \"C - Good\"\n",
        "        elif overall_score >= 60:\n",
        "            return \"D - Fair\"\n",
        "        else:\n",
        "            return \"F - Needs Improvement\"\n",
        "\n",
        "print(\"âœ… Production analyzer defined\")"
      ],
      "metadata": {
        "id": "2tTqYn8LVyg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ðŸ“± CELL 10: Demo Functions for Colab\n",
        "# ============================================================================\n",
        "\n",
        "def demo_file_upload():\n",
        "    \"\"\"Demo function for file upload in Colab\"\"\"\n",
        "    if not IN_COLAB:\n",
        "        print(\"This function is designed for Google Colab\")\n",
        "        return None\n",
        "\n",
        "    print(\"ðŸ“ Please upload your resume file (PDF, DOCX, or TXT)\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded\")\n",
        "        return None\n",
        "\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    print(f\"âœ… Uploaded: {filename}\")\n",
        "    return filename\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "import pathlib\n",
        "import os\n",
        "import re\n",
        "\n",
        "def sanitize_filename(filename: str) -> str:\n",
        "    # Keep only alphanumeric, dash, underscore, and dot\n",
        "    safe = re.sub(r'[^A-Za-z0-9._-]', '_', filename)\n",
        "    return safe\n",
        "\n",
        "def demo_analysis(analyzer, filename: str, job_desc: str = None):\n",
        "    try:\n",
        "        # Ensure absolute path\n",
        "        filename = os.path.abspath(filename)\n",
        "\n",
        "        # Safe name for display/logging only\n",
        "        display_name = re.sub(r'[?\\\\]', '', os.path.basename(filename))\n",
        "\n",
        "        print(f\"ðŸ” Analyzing resume: {display_name}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Pass job description if available\n",
        "        if job_desc:\n",
        "            result = analyzer.analyze(filename, job_description=job_desc)\n",
        "        else:\n",
        "            result = analyzer.analyze(filename)\n",
        "\n",
        "        if not result['success']:\n",
        "            print(f\"âŒ Analysis failed: {result.get('error', 'Unknown error')}\")\n",
        "            return result\n",
        "\n",
        "        # Display results\n",
        "        overall_score = result['scores']['overall']\n",
        "        grade = analyzer.get_ats_grade(overall_score)\n",
        "        print(f\"ðŸŽ¯ Overall ATS Score: {overall_score:.1f}/100 ({grade})\\n\")\n",
        "\n",
        "        print(\"ðŸ“ˆ Component Scores:\")\n",
        "        for component, score in result['scores'].items():\n",
        "            if component != 'overall':\n",
        "                print(f\"   {component.title()}: {score:.1f}/100\")\n",
        "        print()\n",
        "\n",
        "        info = result['document_info']\n",
        "        print(\"ðŸ“„ Document Information:\")\n",
        "        print(f\"   Word Count: {info['word_count']}\")\n",
        "        print(f\"   Sections Found: {', '.join(info['sections_found'])}\")\n",
        "        print(f\"   Contact Info: {list(info['contact_info'].keys())}\\n\")\n",
        "\n",
        "        strengths = result['analysis']['strengths']\n",
        "        if strengths:\n",
        "            print(\"âœ… Strengths:\")\n",
        "            for strength in strengths[:3]:\n",
        "                print(f\"   â€¢ {strength}\")\n",
        "            print()\n",
        "\n",
        "        weaknesses = result['analysis']['weaknesses']\n",
        "        if weaknesses:\n",
        "            print(\"âš ï¸ Areas for Improvement:\")\n",
        "            for weakness in weaknesses[:3]:\n",
        "                print(f\"   â€¢ {weakness}\")\n",
        "            print()\n",
        "\n",
        "        recommendations = result['recommendations']\n",
        "        if recommendations:\n",
        "            print(\"ðŸ’¡ Recommendations:\")\n",
        "            for i, rec in enumerate(recommendations[:2], 1):\n",
        "                print(f\"   {i}. {rec['action']} ({rec['priority']} Priority)\")\n",
        "                for detail in rec['details'][:2]:\n",
        "                    print(f\"      - {detail}\")\n",
        "            print()\n",
        "\n",
        "        print(\"âœ… Analysis complete!\")\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Analysis failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_sample_resume_file():\n",
        "    \"\"\"Create a sample resume file for testing\"\"\"\n",
        "    sample_resume = \"\"\"\n",
        "John Doe\n",
        "Software Engineer\n",
        "Email: john.doe@email.com\n",
        "Phone: (555) 123-4567\n",
        "LinkedIn: linkedin.com/in/johndoe\n",
        "\n",
        "PROFESSIONAL SUMMARY\n",
        "Experienced Software Engineer with 5 years of expertise in Python, JavaScript, and cloud technologies.\n",
        "Passionate about building scalable applications and leading development teams.\n",
        "\n",
        "WORK EXPERIENCE\n",
        "Senior Software Engineer - TechCorp Inc. (2021-2024)\n",
        "â€¢ Led development of microservices architecture serving 1M+ users\n",
        "â€¢ Improved application performance by 40% through code optimization\n",
        "â€¢ Managed team of 6 developers using Agile methodologies\n",
        "â€¢ Technologies: Python, React, AWS, Docker, PostgreSQL\n",
        "\n",
        "Software Developer - StartupXYZ (2019-2021)\n",
        "â€¢ Developed full-stack web applications using Python and JavaScript\n",
        "â€¢ Implemented CI/CD pipelines reducing deployment time by 60%\n",
        "â€¢ Collaborated with cross-functional teams to deliver features\n",
        "â€¢ Technologies: Django, React, MySQL, Jenkins\n",
        "\n",
        "EDUCATION\n",
        "Bachelor of Science in Computer Science\n",
        "State University - Graduated 2019\n",
        "GPA: 3.7/4.0\n",
        "Relevant Coursework: Data Structures, Algorithms, Database Systems\n",
        "\n",
        "TECHNICAL SKILLS\n",
        "Programming Languages: Python, JavaScript, Java, SQL\n",
        "Frameworks: Django, React, Node.js, Express\n",
        "Cloud & DevOps: AWS, Docker, Kubernetes, Jenkins\n",
        "Databases: PostgreSQL, MySQL, MongoDB\n",
        "Tools: Git, Jira, Slack\n",
        "\n",
        "ACHIEVEMENTS\n",
        "â€¢ AWS Certified Solutions Architect\n",
        "â€¢ Published 3 technical articles on Medium\n",
        "â€¢ Winner of company hackathon 2022\n",
        "â€¢ Mentored 8 junior developers\n",
        "\n",
        "PROJECTS\n",
        "E-commerce Platform (2023)\n",
        "â€¢ Built scalable e-commerce platform handling 10K+ daily transactions\n",
        "â€¢ Technologies: Python, React, AWS, PostgreSQL\n",
        "\n",
        "Task Management App (2022)\n",
        "â€¢ Developed real-time collaboration tool for remote teams\n",
        "â€¢ Technologies: Node.js, React, Socket.io, MongoDB\n",
        "\"\"\"\n",
        "\n",
        "    # Save to file\n",
        "    filename = \"sample_resume.txt\"\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(sample_resume)\n",
        "\n",
        "    print(f\"âœ… Created sample resume: {filename}\")\n",
        "    return filename\n",
        "\n",
        "print(\"âœ… Demo functions defined\")\n"
      ],
      "metadata": {
        "id": "HIv8-6N6V6N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# ðŸ“± CELL 12: Main Execution & Testing\n",
        "# ============================================================================\n",
        "\n",
        "def main_colab_demo():\n",
        "    \"\"\"Main demo function for Colab\"\"\"\n",
        "    print(\"ðŸŽ‰ Welcome to AI Resume Analyzer - Colab Edition!\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Train model (if not exists)\n",
        "    model_path = os.path.join(config.MODEL_DIR, \"colab_model.pth\")\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"ðŸ‹ï¸ No trained model found. Starting training...\")\n",
        "        try:\n",
        "            model, dataset, trainer = train_colab_model(max_samples=200, epochs=10, batch_size=4)\n",
        "            print(\"âœ… Training completed!\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Training failed: {e}\")\n",
        "            return\n",
        "    else:\n",
        "        print(\"âœ… Found existing trained model\")\n",
        "\n",
        "    # Step 2: Initialize analyzer\n",
        "    print(\"\\nðŸ”§ Initializing analyzer...\")\n",
        "    scaler_path = os.path.join(config.MODEL_DIR, \"scaler.pkl\")\n",
        "    global analyzer\n",
        "    analyzer = ColabResumeAnalyzer(model_path, scaler_path)\n",
        "\n",
        "    # Step 3: Create sample resume for demo\n",
        "    print(\"\\nðŸ“„ Creating sample resume...\")\n",
        "    sample_file = create_sample_resume_file()\n",
        "\n",
        "    # Step 4: Demo analysis\n",
        "    print(\"\\nðŸ” Running demo analysis...\")\n",
        "    job_desc = \"Software Engineer with Python, AWS, and React experience. Looking for someone with 3+ years experience in web development.\"\n",
        "\n",
        "    result = demo_analysis(analyzer, sample_file, job_desc)\n",
        "\n",
        "    # Step 5: Show next steps\n",
        "    print(\"\\nðŸš€ Next Steps:\")\n",
        "    print(\"1. Upload your own resume using demo_file_upload()\")\n",
        "    print(\"2. Analyze it with demo_analysis(analyzer, filename)\")\n",
        "    print(\"3. Start the API server with run_colab_api()\")\n",
        "    print(\"4. Train on more data with train_colab_model()\")\n",
        "\n",
        "    return analyzer\n",
        "\n",
        "# Quick test function\n",
        "def quick_test():\n",
        "    \"\"\"Quick test of all components\"\"\"\n",
        "    print(\"ðŸ§ª Running quick component test...\")\n",
        "\n",
        "    # Test parser\n",
        "    sample_text = \"John Doe\\nSoftware Engineer\\njohn@email.com\\nPython, Java, AWS# Google Colab Compatible - Enhanced AI Resume Analyzer\"\n",
        "# Run this in Google Colab for best experience\n"
      ],
      "metadata": {
        "id": "iEHY7FwoWGEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_colab_model()"
      ],
      "metadata": {
        "id": "TPNh8iVHWLkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_colab_demo()"
      ],
      "metadata": {
        "id": "ZGmA2N6AXOFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = demo_file_upload()\n",
        "\n",
        "# Use the global analyzer instance initialized below\n",
        "demo_analysis(analyzer, filename,job_desc=\"healthcare\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CCdPPT-KWQUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python deploy.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ4fuUF5ZyBe",
        "outputId": "4137e428-e870-4ae3-e90a-917b6852eec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Starting deployment script...\n",
            "âœ… Deployment structure created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "kH4T8NapdXTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If using Sentence Transformers\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Save your model state dictionary and scaler in the training pipeline (already done in cell gfP4q_ODVsyw)\n",
        "# model.save('exported_model')  # This is not needed for saving your custom scorer model\n",
        "\n",
        "print(\"Model state dictionary and scaler are already saved in the training pipeline.\")\n",
        "print(f\"Model state dict saved to: {os.path.join(config.MODEL_DIR, 'colab_model.pth')}\")\n",
        "print(f\"Scaler saved to: {os.path.join(config.MODEL_DIR, 'scaler.pkl')}\")"
      ],
      "metadata": {
        "id": "W3dooJWvmmKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2926457-8033-4244-c53e-21266eeb77f2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model state dictionary and scaler are already saved in the training pipeline.\n",
            "Model state dict saved to: /content/drive/My Drive/Resume_Analyzer/models/colab_model.pth\n",
            "Scaler saved to: /content/drive/My Drive/Resume_Analyzer/models/scaler.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Config paths\n",
        "# Assuming your notebook is running in the project directory,\n",
        "# paths should be relative or use the config.MODEL_DIR\n",
        "MODEL_PATH = os.path.join('models', 'best_model.pth') # Changed to load best_model.pth\n",
        "SCALER_PATH = os.path.join('models', 'scaler.pkl')\n",
        "\n",
        "# Check if paths exist (important for deployment)\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}\")\n",
        "if not os.path.exists(SCALER_PATH):\n",
        "    raise FileNotFoundError(f\"Scaler file not found at {SCALER_PATH}\")\n",
        "\n",
        "\n",
        "# Load your trained PyTorch model\n",
        "# Use the correct model class definition\n",
        "# class ResumeAnalyzerModel(torch.nn.Module): # Removed\n",
        "#     def __init__(self, input_dim, output_dim): # Removed\n",
        "#         super().__init__() # Removed\n",
        "#         self.fc1 = torch.nn.Linear(input_dim, 128) # Removed\n",
        "#         self.relu = torch.nn.ReLU() # Removed\n",
        "#         self.fc2 = torch.nn.Linear(128, output_dim) # Removed\n",
        "\n",
        "#     def forward(self, x): # Removed\n",
        "#         x = self.relu(self.fc1(x)) # Removed\n",
        "#         x = self.fc2(x) # Removed\n",
        "\n",
        "\n",
        "# Initialize the model (must match training architecture)\n",
        "# input_dim = 300  # Example, match your actual feature size # Removed\n",
        "# output_dim = 1   # Example, adjust based on your scorer # Removed\n",
        "\n",
        "# Initialize the actual trained model class\n",
        "model = ColabResumeScorer() # Use the correct class\n",
        "\n",
        "# Load the state dictionary\n",
        "# Ensure map_location is set correctly (e.g., 'cpu' for deployment without GPU)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))\n",
        "model.eval() # Set model to evaluation mode\n",
        "\n",
        "# Load the scaler\n",
        "with open(SCALER_PATH, 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "\n",
        "print(\"âœ… Model and scaler loaded successfully for deployment.\")\n",
        "print(f\"Model path: {MODEL_PATH}\")\n",
        "print(f\"Scaler path: {SCALER_PATH}\")"
      ],
      "metadata": {
        "id": "5pZFPIpLnOp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1475f1-4df7-42f8-8958-493f06cc0218"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model and scaler loaded successfully for deployment.\n",
            "Model path: models/best_model.pth\n",
            "Scaler path: models/scaler.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9BaFDx44oUx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cad085aa",
        "outputId": "83c65dc8-4894-4528-bc67-4214b5ff3ec2"
      },
      "source": [
        "!pip install pyngrok"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.4.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afd3a87c"
      },
      "source": [
        "Now, run the cell below to start the FastAPI server."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f3d9558"
      },
      "source": [
        "!ngrok authtoken 333J26buLEDBpdXAi7ipeSEqe8c_4jk3RBjYQagdJv6XQW8Bk\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy.py\n",
        "import os\n",
        "import sys\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from fastapi import FastAPI, File, UploadFile, Form, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, Dict, List, Any\n",
        "\n",
        "# Add project directory to sys.path to import modules\n",
        "# Use the PROJECT_DIR variable defined in the notebook's first cell\n",
        "# Assuming this script is intended to be run in the context where PROJECT_DIR is available\n",
        "# If not, you might need to define PROJECT_DIR here or pass it as an environment variable\n",
        "try:\n",
        "    # Try accessing PROJECT_DIR if running in notebook context\n",
        "    from __main__ import PROJECT_DIR\n",
        "except ImportError:\n",
        "    # Fallback if running standalone (e.g., actual deployment)\n",
        "    # You might set this via env var or a config file in a real deployment scenario\n",
        "    PROJECT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "    # Ensure this fallback only happens in standalone mode where __file__ is defined\n",
        "\n",
        "sys.path.insert(0, PROJECT_DIR)\n",
        "\n",
        "\n",
        "import torch\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "# Import your defined classes and config\n",
        "# Ensure these imports match your notebook cells\n",
        "# Import from __main__ if running in notebook\n",
        "# Or import directly if running as a standalone script (requires these files to be present)\n",
        "try:\n",
        "    from __main__ import ColabDocumentParser, ColabFeatureExtractor, ColabResumeScorer, ColabConfig, config, StandardScaler\n",
        "except ImportError:\n",
        "    # Fallback imports for standalone script mode (if you save notebook classes as .py files)\n",
        "    # from your_module import ColabDocumentParser, ColabFeatureExtractor, ColabResumeScorer, ColabConfig, config\n",
        "    # from sklearn.preprocessing import StandardScaler # Assuming StandardScaler is imported here\n",
        "    raise ImportError(\"Could not import necessary classes. Ensure you are running this in the notebook context or have saved notebook classes to .py files in PROJECT_DIR.\")\n",
        "\n",
        "\n",
        "# Apply nest_asyncio for Colab environments if needed (usually handled in main notebook)\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "# --- Load Model and Scaler ---\n",
        "# Config paths - use the paths defined in the notebook config\n",
        "MODEL_PATH = os.path.join(config.MODEL_DIR, 'best_model.pth')\n",
        "SCALER_PATH = os.path.join(config.MODEL_DIR, 'scaler.pkl')\n",
        "\n",
        "# Check if paths exist\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}\")\n",
        "if not os.path.exists(SCALER_PATH):\n",
        "    raise FileNotFoundError(f\"Scaler file not found at {SCALER_PATH}\")\n",
        "\n",
        "# Initialize the model\n",
        "# Use the same device as configured\n",
        "device = config.DEVICE\n",
        "model = ColabResumeScorer().to(device)\n",
        "\n",
        "# Load the state dictionary\n",
        "# Use map_location to load onto the correct device\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model.eval() # Set model to evaluation mode\n",
        "\n",
        "# Load the scaler\n",
        "with open(SCALER_PATH, 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "print(\"âœ… Model and scaler loaded successfully for FastAPI.\")\n",
        "\n",
        "# --- Initialize Components ---\n",
        "parser = ColabDocumentParser()\n",
        "extractor = ColabFeatureExtractor()\n",
        "\n",
        "# --- FastAPI Application ---\n",
        "app = FastAPI(\n",
        "    title=\"AI Resume Analyzer API\",\n",
        "    description=\"Analyzes and scores resumes using a trained deep learning model.\",\n",
        "    version=\"1.0.0\",\n",
        ")\n",
        "\n",
        "# --- API Request and Response Models ---\n",
        "\n",
        "class AnalysisResult(BaseModel):\n",
        "    success: bool\n",
        "    filename: str\n",
        "    scores: Dict[str, float]\n",
        "    analysis: Dict[str, Any]\n",
        "    recommendations: List[Dict[str, Any]]\n",
        "    document_info: Dict[str, Any]\n",
        "    error: Optional[str] = None\n",
        "\n",
        "# Helper function to get ATS grade (copied from notebook)\n",
        "def get_ats_grade(overall_score):\n",
        "    if overall_score >= 90:\n",
        "        return \"A - Excellent\"\n",
        "    elif overall_score >= 80:\n",
        "        return \"B - Very Good\"\n",
        "    elif overall_score >= 70:\n",
        "        return \"C - Good\"\n",
        "    elif overall_score >= 60:\n",
        "        return \"D - Fair\"\n",
        "    else:\n",
        "        return \"F - Needs Improvement\"\n",
        "\n",
        "# Helper function to prepare model inputs (copied/adapted from notebook)\n",
        "def prepare_model_inputs_for_api(features: Dict[str, Any], scaler: StandardScaler, device: torch.device):\n",
        "    \"\"\"Prepare features for model input in the API context\"\"\"\n",
        "    # Extract embeddings\n",
        "    embeddings = {}\n",
        "    for emb_type in ['document', 'experience', 'education', 'skills']:\n",
        "        key = f'{emb_type}_embedding'\n",
        "        emb = features.get(key, torch.zeros(config.EMBEDDING_DIM)) # Use torch zeros directly\n",
        "        # Ensure it's a tensor and unsqueeze for batch dim\n",
        "        if not isinstance(emb, torch.Tensor):\n",
        "             emb = torch.tensor(emb, dtype=torch.float32)\n",
        "\n",
        "        embeddings[emb_type] = emb.unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "    # Extract numeric features\n",
        "    numeric_keys = [\n",
        "        'word_count', 'section_count', 'experience_years', 'education_level',\n",
        "        'total_skills', 'skill_diversity', 'contact_completeness', 'section_completeness',\n",
        "        'ats_friendliness', 'readability_score', 'keyword_density',\n",
        "        'has_email', 'has_phone', 'has_linkedin'\n",
        "    ]\n",
        "\n",
        "    numeric_values = []\n",
        "    for key in numeric_keys:\n",
        "        value = features.get(key, 0.0)\n",
        "        # Apply same caps as in training\n",
        "        if key == 'word_count':\n",
        "            value = min(value, 2000)\n",
        "        elif key == 'experience_years':\n",
        "            value = min(value, 20)\n",
        "        elif key == 'total_skills':\n",
        "            value = min(value, 30)\n",
        "        numeric_values.append(float(value))\n",
        "\n",
        "    numeric_tensor = torch.tensor(numeric_values, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    # Scale if scaler available\n",
        "    if scaler:\n",
        "        numeric_np = numeric_tensor.numpy()\n",
        "        numeric_scaled = scaler.transform(numeric_np)\n",
        "        numeric_tensor = torch.tensor(numeric_scaled, dtype=torch.float32)\n",
        "\n",
        "    numeric_tensor = numeric_tensor.to(device)\n",
        "\n",
        "    return {'embeddings': embeddings, 'numeric': numeric_tensor}\n",
        "\n",
        "\n",
        "# Helper function to generate analysis (copied/adapted from notebook)\n",
        "def generate_analysis_for_api(features, scores, document):\n",
        "    \"\"\"Generate comprehensive analysis for API response\"\"\"\n",
        "    analysis = {\n",
        "        'strengths': [],\n",
        "        'weaknesses': [],\n",
        "        'key_metrics': {}\n",
        "    }\n",
        "\n",
        "    # Identify strengths and weaknesses\n",
        "    for component, score in scores.items():\n",
        "        if component == 'overall':\n",
        "            continue\n",
        "\n",
        "        if score >= 80:\n",
        "            analysis['strengths'].append(f\"Excellent {component} section (Score: {score:.1f})\")\n",
        "        elif score >= 65:\n",
        "            analysis['strengths'].append(f\"Good {component} presentation (Score: {score:.1f})\")\n",
        "        elif score < 50:\n",
        "            analysis['weaknesses'].append(f\"{component.title()} section needs improvement (Score: {score:.1f})\")\n",
        "        elif score < 65:\n",
        "            analysis['weaknesses'].append(f\"{component.title()} could be enhanced (Score: {score:.1f})\")\n",
        "\n",
        "    # Key metrics\n",
        "    analysis['key_metrics'] = {\n",
        "        'word_count': features.get('word_count', 0),\n",
        "        'experience_years': features.get('experience_years', 0),\n",
        "        'total_skills': features.get('total_skills', 0),\n",
        "        'ats_friendliness': f\"{features.get('ats_friendliness', 0) * 100:.1f}%\",\n",
        "        'sections_found': len(features.get('sections_found', [])),\n",
        "        'contact_complete': features.get('contact_completeness', 0) > 0.5\n",
        "    }\n",
        "\n",
        "    # Add ATS Grade\n",
        "    overall_score = scores.get('overall', 0)\n",
        "    analysis['ats_grade'] = get_ats_grade(overall_score)\n",
        "\n",
        "\n",
        "    return analysis\n",
        "\n",
        "# Helper function to generate recommendations (copied/adapted from notebook)\n",
        "def generate_recommendations_for_api(features, scores, job_description):\n",
        "    \"\"\"Generate actionable recommendations for API response\"\"\"\n",
        "    recommendations = []\n",
        "\n",
        "    # Skills recommendations\n",
        "    if scores.get('skills', 0) < 70:\n",
        "        recommendations.append({\n",
        "            'category': 'Skills',\n",
        "            'priority': 'HIGH',\n",
        "            'action': 'Enhance technical skills section',\n",
        "            'details': [\n",
        "                'Add more relevant programming languages',\n",
        "                'Include industry-specific tools',\n",
        "                'Group skills by category',\n",
        "                'Add proficiency levels'\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    # Experience recommendations\n",
        "    if scores.get('experience', 0) < 70:\n",
        "        recommendations.append({\n",
        "            'category': 'Experience',\n",
        "            'priority': 'HIGH',\n",
        "            'action': 'Improve experience descriptions',\n",
        "            'details': [\n",
        "                'Use action verbs to start bullet points',\n",
        "                'Quantify achievements with numbers',\n",
        "                'Show career progression',\n",
        "                'Include relevant keywords'\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    # Education recommendations\n",
        "    if scores.get('education', 0) < 60:\n",
        "        recommendations.append({\n",
        "            'category': 'Education',\n",
        "            'priority': 'MEDIUM',\n",
        "            'action': 'Strengthen education section',\n",
        "            'details': [\n",
        "                'Include relevant coursework',\n",
        "                'Add academic achievements',\n",
        "                'Mention certifications',\n",
        "                'Include graduation honors'\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    # Formatting recommendations\n",
        "    if scores.get('formatting', 0) < 65:\n",
        "        recommendations.append({\n",
        "                'category': 'Formatting',\n",
        "            'priority': 'MEDIUM',\n",
        "            'action': 'Improve ATS compatibility',\n",
        "            'details': [\n",
        "                'Use standard section headers',\n",
        "                'Avoid complex formatting',\n",
        "                'Ensure text is selectable',\n",
        "                    'Use consistent formatting'\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    # Job matching\n",
        "    if job_description and features.get('keyword_density', 0) < 0.2:\n",
        "        recommendations.append({\n",
        "            'category': 'Job Matching',\n",
        "            'priority': 'HIGH',\n",
        "            'action': 'Better align with job requirements',\n",
        "            'details': [\n",
        "                'Include more job-specific keywords',\n",
        "                'Highlight relevant experience',\n",
        "                'Customize skills for this role',\n",
        "                'Match required qualifications'\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "\n",
        "# --- API Endpoint ---\n",
        "\n",
        "@app.post(\"/analyze_resume/\", response_model=AnalysisResult)\n",
        "async def analyze_resume(file: UploadFile = File(...), job_description: Optional[str] = Form(None)):\n",
        "    \"\"\"\n",
        "    Analyzes an uploaded resume file and provides scores, analysis, and recommendations.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        file_content = await file.read()\n",
        "        filename = file.filename\n",
        "\n",
        "        # Step 1: Parse document\n",
        "        document = parser.parse_document(file_content, filename)\n",
        "\n",
        "        if not document.get('success', False):\n",
        "            return AnalysisResult(\n",
        "                success=False,\n",
        "                filename=filename,\n",
        "                scores={},\n",
        "                analysis={},\n",
        "                recommendations=[],\n",
        "                document_info={},\n",
        "                error=document.get('error', 'Unknown parsing error')\n",
        "            )\n",
        "\n",
        "        # Step 2: Extract features\n",
        "        features = extractor.extract_all_features(document, job_description or \"\")\n",
        "\n",
        "        # Step 3: Prepare model inputs\n",
        "        model_inputs = prepare_model_inputs_for_api(features, scaler, device)\n",
        "\n",
        "        # Step 4: Get predictions\n",
        "        with torch.no_grad():\n",
        "            predictions, _ = model(\n",
        "                model_inputs['embeddings'],\n",
        "                model_inputs['numeric']\n",
        "            )\n",
        "\n",
        "        # Step 5: Convert to scores (0-100)\n",
        "        scores = {}\n",
        "        for key, value in predictions.items():\n",
        "            scores[key] = float(value.cpu().item() * 100)\n",
        "\n",
        "        # Step 6: Generate analysis\n",
        "        analysis = generate_analysis_for_api(features, scores, document)\n",
        "\n",
        "        # Step 7: Generate recommendations\n",
        "        recommendations = generate_recommendations_for_api(features, scores, job_description or \"\")\n",
        "\n",
        "\n",
        "        return AnalysisResult(\n",
        "            success=True,\n",
        "            filename=filename,\n",
        "            scores=scores,\n",
        "            analysis=analysis,\n",
        "            recommendations=recommendations,\n",
        "            document_info={\n",
        "                'word_count': document.get('word_count', 0),\n",
        "                'sections_found': document.get('sections', []),\n",
        "                'contact_info': document.get('contact', {})\n",
        "            }\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise HTTPException(status_code=500, detail=f\"Internal Server Error: {e}\")\n",
        "\n",
        "# --- Run the API (for Colab) ---\n",
        "# This part is typically run directly in the deploy.py script execution cell\n",
        "# In a standard deployment, you might use a command like `uvicorn deploy:app --reload`\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if running in Colab (optional, can be removed for standard deployment)\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "\n",
        "    if IN_COLAB:\n",
        "        # Use ngrok for public URL in Colab\n",
        "        from pyngrok import ngrok\n",
        "        import asyncio\n",
        "\n",
        "        # Terminate any existing ngrok tunnels\n",
        "        print(\"Terminating existing ngrok tunnels...\")\n",
        "        ngrok.kill()\n",
        "\n",
        "        # --- Add this line with your ngrok authtoken ---\n",
        "        # You can get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "        # For security, it's recommended to store your token in Colab Secrets Manager (key icon)\n",
        "        # and access it like: ngrok.set_auth_token(userdata.get('NGROK_AUTH_TOKEN'))\n",
        "        # from google.colab import userdata # Uncomment if using Secrets Manager\n",
        "        ngrok_auth_token = \"333J26buLEDBpdXAi7ipeSEqe8c_4jk3RBjYQagdJv6XQW8Bk\" # <<< REPLACE WITH YOUR ACTUAL AUTH TOKEN\n",
        "        # ngrok_auth_token = userdata.get('NGROK_AUTH_TOKEN') # <<< Use this if using Secrets Manager\n",
        "        if ngrok_auth_token == \"333J26buLEDBpdXAi7ipeSEqe8c_4jk3RBjYQagdJv6XQW8Bk\":\n",
        "            print(\"\\nâš ï¸ IMPORTANT: Please replace 'YOUR_AUTH_TOKEN' with your actual ngrok authtoken!\")\n",
        "            print(\"Sign up: https://dashboard.ngrok.com/signup\")\n",
        "            print(\"Get token: https://dashboard.ngrok.com/get-started/your-authtoken\\n\")\n",
        "        else:\n",
        "             ngrok.set_auth_token(ngrok_auth_token)\n",
        "\n",
        "        # Start ngrok tunnel\n",
        "        print(\"Starting ngrok tunnel...\")\n",
        "        public_url = ngrok.connect(8000).public_url\n",
        "        print(f\"âœ¨ FastAPI app running on: {public_url}/docs\")\n",
        "\n",
        "        # Run uvicorn server\n",
        "        # Use asyncio.run if not already in an event loop (Colab often is)\n",
        "        # Or use nest_asyncio if required\n",
        "        try:\n",
        "             uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "        except RuntimeError as e:\n",
        "             if \"Event loop is running\" in str(e):\n",
        "                  print(\"Event loop is already running, applying nest_asyncio and restarting uvicorn...\")\n",
        "                  nest_asyncio.apply()\n",
        "                  uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "             else:\n",
        "                  raise # Re-raise other RuntimeErrors\n",
        "    else:\n",
        "        # Standard local execution\n",
        "        print(\"Running locally...\")\n",
        "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GZIRRmaqoAI",
        "outputId": "60a06988-9581-416a-f1b6-1b6ffea0e43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-14' coro=<Server.serve() done, defined at /usr/local/lib/python3.12/dist-packages/uvicorn/server.py:69> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 396, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
            "    self.__step_run_and_handle_result(exc)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model and scaler loaded successfully for FastAPI.\n",
            "Terminating existing ngrok tunnels...\n",
            "\n",
            "âš ï¸ IMPORTANT: Please replace 'YOUR_AUTH_TOKEN' with your actual ngrok authtoken!\n",
            "Sign up: https://dashboard.ngrok.com/signup\n",
            "Get token: https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "\n",
            "Starting ngrok tunnel...\n",
            "âœ¨ FastAPI app running on: https://3652686c8c48.ngrok-free.app/docs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [17940]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2vEZIhCOrO_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}